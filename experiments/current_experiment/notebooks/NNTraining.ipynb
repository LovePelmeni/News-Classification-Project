{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas \n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import warnings\n",
    "import os \n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "from catalyst.utils import set_global_seed, prepare_cudnn\n",
    "from catalyst.dl import AccuracyCallback, OptimizerCallback, CheckpointCallback\n",
    "from catalyst.dl import SupervisedRunner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 512 \n",
    "MODEL_NAME = \"distilbert-base-uncased\"\n",
    "SEED = 0.10\n",
    "ACCUM_STEPS=1 \n",
    "F16_PARAMS=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_classification import text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = pandas.read_csv(\"../data/processed_data/training_set.csv\")\n",
    "validation_set = pandas.read_csv(\"../data/processed_data/validation_set.csv\")\n",
    "testing_set = pandas.read_csv(\"../data/processed_data/testing_set.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_true = testing_set['category']\n",
    "testing_set.drop(columns=['category'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constructing text datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = text.TextClassificationDataset(\n",
    "    labels=training_set['category'].values.tolist(),\n",
    "    texts=training_set['text'].values.tolist(),\n",
    "    max_sequence_length=MAX_SEQUENCE_LENGTH,\n",
    "    model_name=MODEL_NAME,\n",
    "    label_dict=None\n",
    ")\n",
    "\n",
    "validation_set = text.TextClassificationDataset(\n",
    "    labels=validation_set['category'].values.tolist(),\n",
    "    texts=validation_set['text'].values.tolist(),\n",
    "    max_sequence_length=MAX_SEQUENCE_LENGTH,\n",
    "    model_name=MODEL_NAME,\n",
    "    label_dict=training_set.label_dict\n",
    ") \n",
    "\n",
    "testing_set = text.TextClassificationDataset(\n",
    "    labels=testing_set['category'].values.tolist(),\n",
    "    texts=testing_set['text'].values.tolist(),\n",
    "    max_sequence_length=MAX_SEQUENCE_LENGTH,\n",
    "    model_name=MODEL_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loaders = {\n",
    "    'train': DataLoader(dataset=training_set, shuffle=True),\n",
    "    'valid': DataLoader(dataset=validation_set, shuffle=True),\n",
    "    'test': DataLoader(dataset=testing_set, shuffle=True)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing Bert Text Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = training_set['category'].unique()\n",
    "LEARNING_RATE = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = text.BertTextClassifier(num_classes=NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(classifier.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"   \n",
    "set_global_seed(SEED)                       \n",
    "prepare_cudnn(deterministic=True)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training model using catalyst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "runner = SupervisedRunner(\n",
    "    input_key=(\n",
    "        \"features\",\n",
    "        \"attention_mask\"\n",
    "    )\n",
    ")\n",
    "\n",
    "runner.train(\n",
    "    model=classifier,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    loaders=data_loaders,\n",
    "    callbacks=[\n",
    "        AccuracyCallback(num_classes=NUM_CLASSES),\n",
    "#       F1ScoreCallback(activation='Softmax'), # Tried it, but got an error on tensor shape\n",
    "        OptimizerCallback(accumulation_steps=ACCUM_STEPS)\n",
    "    ],\n",
    "    fp16=FP16_PARAMS,\n",
    "    logdir=\"../nn_log\"\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaludating Neural Network Performance using Testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loaders = {\n",
    "    \"test\": DataLoader(\n",
    "        dataset=testing_set,\n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=False\n",
    "    ) \n",
    "}\n",
    "\n",
    "runner.infer(\n",
    "    model=classifier,\n",
    "    loaders=test_loaders,\n",
    "    callbacks=[\n",
    "        CheckpointCallback(\n",
    "            resume=f\"../nn_checkpoints/best.pth\" % ()\n",
    "        ),\n",
    "        InferCallback(),\n",
    "    ],   \n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating Neural Network results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = runner.state.callbacks[0].predictions['logits']\n",
    "\n",
    "test_pred = pandas.DataFrame(\n",
    "    {\n",
    "    'label': probs.argmax(axis=1)\n",
    "    }\n",
    ")\n",
    "\n",
    "test_pred['label'] = test_pred['label'].map(\n",
    "    {\n",
    "        key: value for key, value in training_set.text.items()\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing Heatmap Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "ax = sns.heatmap(cmtx, annot=True)\n",
    "ax.set_xlabel(\"Target\")\n",
    "ax.set_ylabel(\"Predicted\")\n",
    "ax.set_title('Testing set - Confusion Matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating Testing predictions using evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, recall_score, f1_score\n",
    " \n",
    "recall = recall_score(y_true=test_true, y_pred=test_pred, average='weighted')\n",
    "accuracy = accuracy_score(y_true=test_true, y_pred=test_pred)\n",
    "f1 = f1_score(y_true=test_true, y_pred=test_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('average weighted recall: ', recall)\n",
    "print('accuracy score: ', accuracy)\n",
    "print('f1 score: ', f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
