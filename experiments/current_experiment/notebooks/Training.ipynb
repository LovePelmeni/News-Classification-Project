{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models import train_model\n",
    "import pandas\n",
    "import pickle\n",
    "import os\n",
    "from baseline_requirements import metrics \n",
    "from text_classification import text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"../data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading raw dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = pandas.read_csv(os.path.join(DATA_PATH, \"/processed_data/training_set.csv\"))\n",
    "validation_set = pandas.read_csv(os.path.join(DATA_PATH, \"/processed_data/validation_set.csv\"))\n",
    "testing_set = pandas.read_csv(os.path.join(DATA_PATH, \"/processed_data/testing_set.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting text datasets to TF / IDF Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = text.TFIDFVectorizedDataset(text_data=training_set).get_vectorized_df()\n",
    "validation_set = text.TFIDFVectorizedDataset(text_data=validation_set).get_vectorized_df()\n",
    "testing_set = text.TFIDFVectorizedDataset(text_data=testing_set).get_vectorized_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing the best baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import make_scorer\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"lg\": LogisticRegression(),\n",
    "    \"mnb\": MultinomialNB(),\n",
    "    \"svm\": LinearSVC(),\n",
    "}\n",
    "\n",
    "hyperparams = {\n",
    "    \"lg\": {\n",
    "        'penalty' : ['l1', 'l2', 'elasticnet', 'none'], # type of regularization\n",
    "        'C' : numpy.logspace(-4, 4, 20),  # C parameter\n",
    "        'solver' : ['lbfgs','newton-cg','liblinear','sag','saga'],\n",
    "        'max_iter' : [100, 1000,2500, 5000] # maximum number of iterations\n",
    "    },\n",
    "    \"mnb\": {\n",
    "        'alpha': [0.1, 0.5, 1.0, 1.5], # alpha regularization parameter\n",
    "        'fit_prior': [True, False]\n",
    "    },\n",
    "    \"svm\": {\n",
    "        'C': 1.0,  # Regularization strength\n",
    "        'loss': 'squared_hinge',  # Loss function\n",
    "        'dual': [True, False],  # Whether to solve the dual or primal problem\n",
    "        'fit_intercept': [True, False],  # Whether to calculate the intercept\n",
    "        'max_iter': 1000,  # Maximum number of iterations for the solver\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Selection using Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "metric = make_scorer(recall_score(average='macro'), greater_is_better=True)\n",
    "feature_importances = {}\n",
    "\n",
    "X_train = training_set.drop(columns=['category'])\n",
    "Y_train = training_set['category']\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    # Computing Feature importances using Recursive Feature Elimination (RFE)\n",
    "    cv_results = RFECV(\n",
    "        estimator=model,\n",
    "        step=2,\n",
    "        min_features_to_select=4,\n",
    "        cv=StratifiedKFold(n_splits=5),\n",
    "        n_jobs=-1,\n",
    "        scoring=metric,\n",
    "    )\n",
    "    cv_results.fit(X_train, Y_train)\n",
    "\n",
    "    # storing output important features \n",
    "    feature_importances[model_name] = {\n",
    "        'important_features': cv_results.cv_results_\n",
    "    }\n",
    "\n",
    "feature_importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter Tuning using Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    score, best_model = train_model.fine_tune_model(\n",
    "        k_cross=5,\n",
    "        training_set=validation_set,\n",
    "        target_variable=\"category\",\n",
    "        hyperparams=hyperparams[model_name],\n",
    "        model=model,\n",
    "        loss_function_or_scorer_metric=metric,\n",
    "    )\n",
    "    output[model_name] = {\n",
    "        \"best_model\": best_model,\n",
    "        \"best_score\": score\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing output and choosing best baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Picking the best model, based on a given score from the HP output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_model = sorted(output, lambda model: model['best_score'], reverse=True)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing model using cross-validation on Testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "X, Y = testing_set.drop(columns=['category']), testing_set['category']\n",
    "cv = cross_validate(\n",
    "    estimator=chosen_model,\n",
    "    scoring=metric,\n",
    "    X=testing_set,\n",
    "    cv=StratifiedKFold(n_splits=5, shuffle=True)\n",
    ")\n",
    "print('test metric score: %s' % cv['test_score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimating model performance according to baseline metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('meets expected metric: ', metrics.AVERAGED_WEIGHTED_RECALL <= cv['test_score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(chosen_model, open('../models/baseline_classifier.pkl', mode='wb'))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
